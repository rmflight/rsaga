

# SAGA

Simulated Annealing / Genetic Algorithm.

```{r initLib}
library(saga)
```


## Test Functions

As an initial test function, we will use a multi-linear regression that is dependent on 8 different parameters. This function is taken from the [NIST website](http://www.itl.nist.gov/div898/strd/nls/data/LINKS/v-gauss3.shtml).

```{r setUpQuery}
betaValues <- c(9.8940368970E+01,
                1.0945879335E-02,
                1.0069553078E+02,
                1.1163619459E+02,
                2.3300500029E+01,
                7.3705031418E+01,
                1.4776164251E+02,
                1.9668221230E+01)

betaSD <- c(5.3005192833E-01,
            1.2554058911E-04,
            8.1256587317E-01,
            3.5317859757E-01,
            3.6584783023E-01,
            1.2091239082E+00,
            4.0488183351E-01,
            3.7806634336E-01)

fx <- function(x, beta){
  beta[1] * exp(-1 * beta[2] * x) + 
  beta[3] * exp(-1 * (x - beta[4])^2 / beta[5]^2) + 
  beta[6] * exp(-1 * (x - beta[7])^2 / beta[8]^2)
}

x <- seq(1, 250, 1)
y <- fx(x, betaValues)
plot(x, y)
```

Our evaluation criteria is **how** different is a new set of `y` based on our candidate `beta` values. Note that this energy function is specific to our current problem.

```{r energyFunction}
energyFunc <- function(newBeta){
  orgBeta <- c(9.8940368970E+01,
                1.0945879335E-02,
                1.0069553078E+02,
                1.1163619459E+02,
                2.3300500029E+01,
                7.3705031418E+01,
                1.4776164251E+02,
                1.9668221230E+01)
  fx <- function(x, beta){
    beta[1] * exp(-1 * beta[2] * x) + 
    beta[3] * exp(-1 * (x - beta[4])^2 / beta[5]^2) + 
    beta[6] * exp(-1 * (x - beta[7])^2 / beta[8]^2)
  }
  x <- seq(1, 250, 1)
  org <- fx(x, orgBeta)
  new <- fx(x, newBeta)
  
  nObs <- length(org)
  return((sum((org - new)^2)))
}
```


And our values for each value of beta will be drawn uniformly from a distribution with a range of 0 to 200. The neighbor values will be based on the current values, but the standard deviation will be adjusted based on the current temperature and alpha.

```{r initPop}
neighborFunction <- function(currentPopulation, useSD, currentTemperature, alpha){
  # an alpha of 0 means temperature has no effect
  if (alpha == 0){
    scaleVariance <- 1
    tmpSD <- useSD
  } else {
    scaleVariance <- currentTemperature * alpha / length(currentPopulation)
    tmpSD <- useSD * scaleVariance
  }
  newPopulation <- rnorm(length(currentPopulation), mean=currentPopulation, sd=tmpSD)
  newPopulation[newPopulation < 0] <- 0
  return(newPopulation)
}
```


Lets write a simple simulated annealing algorithm.


```{r tryitOut}
library(functional)
initPop <- rnorm(8, mean=betaValues, sd=0.2*betaValues)
initPop[(initPop < 0)] <- 0
useSD <- 0.2 * betaValues

tmpNeighbor <- Curry(neighborFunction, useSD=useSD)

outSA <- sa(initPop, energyFunc, tmpNeighbor, 300, 0.005, alpha=0.01, nTry=Inf)
plot(outSA$allEnergy)
plot(log10(outSA$allEnergy))

y2 <- fx(x, outSA$lastSolution)
plot(x,y)
plot(x, y2)
```

```{r try50}
library(snowfall)
sfInit(parallel=TRUE, cpus=4)

sfLibrary(saga)
sfExport("tmpNeighbor", "energyFunc")

varyInitPop <- lapply(seq(1, 50), function(x){
  rnorm(8, mean=betaValues, sd=0.2*betaValues)
})

outRes <- sfLapply(varyInitPop, function(x){
  sa(x, energyFunc, tmpNeighbor, 300, 0.005, alpha=0.01, nTry=Inf)
})
sfStop()
save(outRes, file="inst/data/sa50Test.RData")
```

```{r checkSASolutions}
bestEnergies <- sapply(outRes, function(x){ x$bestEnergy})
plot(bestEnergies)
bestSol <- lapply(outRes, function(x){x$bestSolution})
bestSol <- do.call(rbind, bestSol)
betaValues
bestSol
plot(log10(bestEnergies))
```


## Nelder-Mead Simplex

Because Numerical Methods originally used SA for our types of problems by combining with a simplex, I want to understand the Nelder-Mead simplex downhill search.

We will test it using a rather trivial example of finding roots to a binomial equation.

```{r findRoots}
evalFunc <- function(xyVals){
  x <- xyVals[1]
  y <- xyVals[2]
  outVal <- x^2 - 4*x + y^2 - y - x*y
  return(outVal)
}
```

```{r initGuess}
usePoints <- list(c(0, 0),
                   c(1.2, 0),
                   c(0.0, 0.8))
```

We evaluate the points!

```{r checkInit}
evalPoints <- sapply(usePoints, evalFunc)
```

And sort them from best to worst!

```{r sortPoints}
sortEval <- order(evalPoints, decreasing=F)
evalPoints <- evalPoints[sortEval]
usePoints <- usePoints[sortEval]
names(usePoints) <- c("b", "g", "w")
```

Calculate the mid-point of best two points:

```{r calcMid}
calcMid <- function(inPoints){
  allPoints <- do.call(rbind, inPoints)
  midPoint <- rowMeans(allPoints)
  return(midPoint)
}
```

```{r getMid}
midPoint <- calcMid(usePoints[1:2])
```

Generate a new reflected, expanded, or contracted point (which is dependent on the coefficient passed in).

```{r calcNew}
calcNewPoint <- function(inMid, inWorst, useCoefficient=1){
  newPoint <- inMid + useCoefficient * (inMid - inWorst)
  names(newPoint) <- NULL
  return(newPoint)
}

scalePoints <- function(inPoint, useCoefficient=0.5){
  return(inPoint * useCoefficient)
}
```

Reflect our current simplex.

```{r reflectSimplex}
refPoint <- calcNewPoint(midPoint, usePoints[[3]])
evalRef <- evalFunc(refPoint)
evalRef
evalPoints[1]
```

Our new point has a better energy than the best point. We should try expanding it!

```{r expandSimplex}
expPoint <- calcNewPoint(midPoint, usePoints[[3]], 2)
evalExp <- evalFunc(expPoint)
evalExp
evalPoints[1]
```

OK, even better. Lets replace the previous best point.

```{r replaceBest}
usePoints[[1]] <- expPoint
```

This function expects a list of points, and a function to evaluate each point. Returns a new simplex.

```{r simplexLogic}
simplexLogic <- function(inPoints, evalFunction){
  nDim <- length(inPoints) - 1
  
  # order the points from best to worst
  inEval <- sapply(inPoints, evalFunction)
  newOrder <- order(inEval, decreasing=FALSE)
  
  inEval <- inEval[newOrder]
  inPoints <- inPoints[newOrder]
  outPoints <- inPoints
  
  midPoint <- calcMid(inPoints[1:nDim])
  
  # try reflection
  refPoint <- calcNewPoint(midPoint, inPoints[[nDim+1]], 1)
  refEval <- evalFunction(refPoint)
  
  if (refEval < inEval[2]){
    if (inEval[2] < refEval){
      outPoints[[3]] <- refPoint
    } else {
      expPoint <- calcNewPoint(midPoint, inPoints[[nDim+1]], 2)
      expEval <- evalFunction(expPoint)
      if (expEval < inEval[1]){
        outPoints[[3]] <- expPoint
      } else {
        outPoints[[3]] <- refPoint
      }
    }
  } else {
    if (refEval < inEval[3]){
      outPoints[[3]] <- refPoint
    }
    contPoint <- calcNewPoint(midPoint, inPoints[[nDim+1]], -0.5)
    contEval <- evalFunction(contPoint)
    
    if (contEval < inEval[3]){
      outPoints[[3]] <- contPoint
    } else {
      outPoints[2:(nDim+1)] <- lapply(inPoints[2:(nDim+1)], scalePoints)
    }
    
    
  }
  return(outPoints)
}
```


## Check that Nelder-Mead works properly

```{r outSurface}
xVals <- seq(-20, 20, 0.1)
yVals <- seq(-20, 20, 0.1)
zVals <- sapply(seq(1, length(xVals)), function(x){evalFunc(c(xVals[x], yVals[x]))})
```

